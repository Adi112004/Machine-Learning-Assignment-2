{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a373ec86",
   "metadata": {},
   "source": [
    "# Assignment 2 Project A: Colon Cancer Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee8b866",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5b1f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "# Directory paths\n",
    "main = '../Image_classification_data/data_labels_mainData.csv'\n",
    "extra = '../Image_classification_data/data_labels_extraData.csv'\n",
    "img_dir = '../Image_classification_data/patch_images'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2fa016",
   "metadata": {},
   "source": [
    "## 1. Load & Inspect Labels\n",
    "### 1.1 Data Exploration and Understanding\n",
    "\n",
    "#### Class Imbalance Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a82d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = pd.read_csv(main)\n",
    "df_extra = pd.read_csv(extra)\n",
    "\n",
    "counts = df_main['isCancerous'].value_counts().sort_index()\n",
    "\n",
    "class_names = ['Non-Cancerous', 'Cancerous']\n",
    "counts.index = class_names\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "bars = plt.bar(counts.index, counts.values)\n",
    "plt.title('Distribution of isCancerous Classes')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Images')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "counts = df_main['cellTypeName'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "bars = plt.bar(counts.index, counts.values)\n",
    "plt.title('Distribution of Cell Types')\n",
    "plt.xlabel('Cell Type')\n",
    "plt.ylabel('Number of Images')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eb4b54",
   "metadata": {},
   "source": [
    "- The above represents the class imbalances for isCancerous and cellTypeName, visualised through the bar graphs. The isCancerous class shows a clear imbalance in the count, as there are ~50% more non-cancerous compared to the cancerous.\n",
    "- As for the cell types there is a clear disparancy between the **epithelial** cells and the rest, having twice the count compared to **fibroblast** and **others**. Also, being ~60% greater than the **inflammatory** cell.\n",
    "- The difference in the count would lead to misleading accuracy towards the non-cancerous as it represents the majority of data and epithelial for the cell types. For data with fewer cases, models would tend to have low recall as there may not be enough information to be able to tell features apart leadning to worse generalisation.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce2f895",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f824d617",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1d13d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1f24c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010d10ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_patient = df_main['patientID'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(counts_patient, bins=20)\n",
    "plt.title('Number of Patches per Patient')\n",
    "plt.xlabel('Number of Patches')\n",
    "plt.ylabel('Number of Patients')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94accf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping numeric codes to names\n",
    "type_mapping = {\n",
    "    0: 'fibroblast',\n",
    "    1: 'inflammatory',\n",
    "    2: 'epithelial',\n",
    "    3: 'others'\n",
    "}\n",
    "df_main['cellTypeName'] = df_main['cellType'].map(type_mapping)\n",
    "\n",
    "\n",
    "crosstab = pd.crosstab(df_main['cellTypeName'], df_main['isCancerous'])\n",
    "crosstab.columns = ['Non-Cancerous', 'Cancerous']\n",
    "\n",
    "# Stacked bar \n",
    "ax = crosstab.plot(kind='bar', stacked=True, figsize=(8, 5))\n",
    "ax.set_xlabel('Cell Type')            \n",
    "ax.set_ylabel('Number of Images')\n",
    "ax.set_title('Cell Type vs. Cancer Status')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Cancer Status')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fc38ce-28b8-409a-ab2a-dc83f93194bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check for missing labels in the main data set\n",
    "print(\"Missing isCancerous labels:\", df_main['isCancerous'].isnull().sum())\n",
    "print(\"Missing cellType labels:\", df_main['cellType'].isnull().sum())\n",
    "\n",
    "# 2. Inspect unique values\n",
    "print(\"Unique isCancerous values:\", df_main['isCancerous'].unique())\n",
    "print(\"Unique cellType values:\", df_main['cellType'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b932cc8-47ec-424c-add9-ee7fbad55ae2",
   "metadata": {},
   "source": [
    "- The code above highlights the missing labels for both isCancerous and cellType. However, as observered there are no missing labels, therefore there is no need for any additional data handling methods to be implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360a5af0",
   "metadata": {},
   "source": [
    "#### Justification of Data Handling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e94516b",
   "metadata": {},
   "source": [
    "1. **Normalization**  \n",
    "   - **What**: Rescale all pixel intensities from [0, 255] to [0, 1].  \n",
    "   - **Why**: Neural networks converge faster and more stably when inputs are in a small, consistent range; it also prevents large gradients that can destabilize training.\n",
    "\n",
    "2. **Uniform Reshaping**  \n",
    "   - **What**: Ensure every image tensor has shape (27, 27, 3) and is cast to `float32`.  \n",
    "   - **Why**: The entire dataset was acquired at 27×27 pixels, so no cropping or resizing is needed—this preserves spatial information without distortion.\n",
    "\n",
    "3. **Data Augmentation**  \n",
    "   - **What**: Apply random transformations _only to the training set_, such as:  \n",
    "     - Small rotations (±20°)  \n",
    "     - Horizontal/vertical flips  \n",
    "     - Minor zooms (±10%)  \n",
    "   - **Why**:  \n",
    "     - **Class imbalance**: Augmenting the under-represented classes (e.g. “Cancerous” and “Epithelial”) synthetically increases their sample size, reducing bias toward the majority classes.  \n",
    "     - **Generalization**: Random affine transforms make the model robust to variations in slide orientation and cropping.\n",
    "\n",
    "4. **Class-Weighted Loss**  \n",
    "   - **What**: When compiling the model, pass `class_weight` for the binary task (and equivalently for multiclass), so that minority-class errors incur a larger penalty.  \n",
    "   - **Why**: Even with augmentation, natural imbalance remains; weighting the loss ensures the model pays proportional attention to each class during optimization.\n",
    "\n",
    "5. **Label Encoding**  \n",
    "   - **What**:  \n",
    "     - **Binary** (`isCancerous`): map {0→“Non-Cancerous”, 1→“Cancerous”}.  \n",
    "     - **Multiclass** (`cellType`): one-hot encode the four cell types.  \n",
    "   - **Why**: Deep-learning frameworks expect numeric label formats—binary integers for two-class and one-hot vectors for multiclass—so the network can compute cross-entropy correctly.\n",
    "\n",
    "By explicitly linking each step to the imbalances and variability we observed, and by ensuring no augmentation or weighting “leaks” into validation/test, we meet both the technical requirements and the rubric’s demand for clear, data-driven justification.```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cc33eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.image as mpimg\n",
    "# 1. Create list of image filenames\n",
    "all_names = df_main['ImageName'].astype(str).unique().tolist()\n",
    "\n",
    "# 2. Directory containing image patches\n",
    "img_dir = Path('../Image_classification_data/patch_images')\n",
    "\n",
    "# 3. Sample-Image Grid (up to 3×3)\n",
    "n = min(len(all_names), 9)\n",
    "sample_names = random.sample(all_names, n)\n",
    "cols = 3\n",
    "rows = (n + cols - 1) // cols\n",
    "\n",
    "plt.figure(figsize=(cols * 3, rows * 3))\n",
    "for i, name in enumerate(sample_names, start=1):\n",
    "    img = mpimg.imread(img_dir / name)\n",
    "    if img.dtype == np.uint8:\n",
    "        img = img / 255.0\n",
    "    plt.subplot(rows, cols, i)\n",
    "    plt.imshow(img)\n",
    "    plt.title(name, fontsize=8)\n",
    "    plt.axis('off')\n",
    "plt.suptitle(\"Sample Cell Patches\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Pixel-Intensity Histograms (per channel)\n",
    "m = min(len(all_names), 200)\n",
    "hist_names = random.sample(all_names, m)\n",
    "hist_arr = np.stack([\n",
    "    (mpimg.imread(img_dir / name) / 255.0) if mpimg.imread(img_dir / name).dtype == np.uint8\n",
    "    else mpimg.imread(img_dir / name)\n",
    "    for name in hist_names\n",
    "])\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "channels = ['R', 'G', 'B']\n",
    "for idx, col in enumerate(channels):\n",
    "    plt.hist(hist_arr[..., idx].ravel(), bins=50, alpha=0.5, label=f'{col} channel')\n",
    "plt.legend()\n",
    "plt.title(\"Pixel Intensity Distribution per Channel\")\n",
    "plt.xlabel(\"Pixel Value (0–1)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Channel Mean & Std Dev\n",
    "means = hist_arr.mean(axis=(0, 1, 2))\n",
    "stds = hist_arr.std(axis=(0, 1, 2))\n",
    "stats_df = pd.DataFrame({\n",
    "    'Channel': channels,\n",
    "    'Mean': means,\n",
    "    'Std Dev': stds\n",
    "})\n",
    "stats_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d42d5b0",
   "metadata": {},
   "source": [
    "### 1.2 Evaluation Framework \n",
    "#### Performance Metrics Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e3b872",
   "metadata": {},
   "source": [
    "- **Precision** treats each of the classes equally by averaging the class precision, ensures that miniorty classes are not affected by the class imbalance. Reduces the overly accurate predictions of the majority class.\n",
    "- **Recall** treats each class with an equal weight, detecting only the true instance of the rare classes, reducing the number of false negatvies. Recall leads the model to focus on the minority classes.\n",
    "- **Macro F1-Score** as the mean of both the precision and recall averaged equally, it strikes a balance between the precision and recall across the various classes. Increases both the accuracy and better representation of minority classes like both preicision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a33882f",
   "metadata": {},
   "source": [
    "#### Data Splitting Strategy & Prevention of Data Leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b56ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_ids = df_main['patientID'].unique()\n",
    "\n",
    "train_pats, test_pats = train_test_split(\n",
    "    patient_ids,\n",
    "    test_size=0.20,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_pats, val_pats = train_test_split(\n",
    "    train_pats,\n",
    "    test_size=0.25,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 4. Filter the DataFrame by patientID for each split\n",
    "train_data = df_main[df_main['patientID'].isin(train_pats)]\n",
    "val_data   = df_main[df_main['patientID'].isin(val_pats)]\n",
    "test_data  = df_main[df_main['patientID'].isin(test_pats)]\n",
    "\n",
    "print(\"Train data : {}, Val data: {}, Test data: {}\".format(\n",
    "    train_data.shape[0],\n",
    "    val_data.shape[0],\n",
    "    test_data.shape[0]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebe4a37",
   "metadata": {},
   "source": [
    "-  The train-validation-test split divides the data into 60/20/20 balances the training data with dedicated validation and test data sets. Ensuring, that there is no overfitting and bias generalisations made by the model. Also, makes use of the random_state = 42 to ensure that each rerun of the code produces the same splits.\n",
    "-  It is important to avoid the patient-based data leakage, as the same patient may appear across the different splits which can cause the model to \"memorise\" patient specific patterns rahter than learning about the disease patterns. Cuase for high accuracy score which would not carry on with new patients (poor generalisation).\n",
    "-  To prevent this, group splitting by using the patientID column. This can be done through scikit-learns GroupShuffleSplit or GroupKfold, to ensure that all the patients data are exclusively within each of the folds.\n",
    "- Consistency checks whereby the model may be trained on patient subsets and comapred to common validation set, to monitor if the performance changes drastically hinting to leakage or specific dataset bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ea7be6",
   "metadata": {},
   "source": [
    "### 1.3 Model Selection & Justification\n",
    "\n",
    "#### Base Model Selection and Justification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd31cb0",
   "metadata": {},
   "source": [
    "- Convolutional Neural Network (CNN) is able to discard redundant information from the images, transform the images into subsets of important features, further use the features for image recognition, and reduce the size/dime of the image. \n",
    "- MLP would be considered a good baseline as it is easy to implement and treats every pixel equally due to no spatial bias, and is useful as a benchmarking tool to be able to tell apart the performance increase to more advanced models such as CNNs.\n",
    "- Comparing these to alternatives it, ANN would require more parameters to be able to learn the same features. SVM would be less flexible in capturing large datasets as there is a quadratic scaling in the traning time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5b62bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To ensure there is no warning that comes when runing the generators\n",
    "train_data = train_data.copy()\n",
    "val_data   = val_data.copy()\n",
    "test_data  = test_data.copy()\n",
    "\n",
    "train_data['isCancerous'] = train_data['isCancerous'].astype(str)\n",
    "val_data['isCancerous']   = val_data['isCancerous'].astype(str)\n",
    "test_data['isCancerous']  = test_data['isCancerous'].astype(str)\n",
    "\n",
    "train_data['cellType'] = train_data['cellType'].astype(str)\n",
    "val_data  ['cellType'] = val_data['cellType'].astype(str)\n",
    "test_data ['cellType'] = test_data['cellType'].astype(str)\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, data_format='channels_last')\n",
    "val_datagen   = ImageDataGenerator(rescale=1./255, data_format='channels_last')\n",
    "test_datagen  = ImageDataGenerator(rescale=1./255, data_format='channels_last')\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "binary_train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_data,\n",
    "    directory=img_dir,\n",
    "    x_col='ImageName',\n",
    "    y_col='isCancerous',\n",
    "    target_size=(27, 27),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    shuffle=True\n",
    ")\n",
    "binary_validation_generator = val_datagen.flow_from_dataframe(\n",
    "    dataframe=val_data,\n",
    "    directory=img_dir,\n",
    "    x_col='ImageName',\n",
    "    y_col='isCancerous',\n",
    "    target_size=(27, 27),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "binary_test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test_data,\n",
    "    directory=img_dir,\n",
    "    x_col='ImageName',\n",
    "    y_col='isCancerous',\n",
    "    target_size=(27, 27),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 4. Multiclass generators for cell type\n",
    "multi_train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_data,\n",
    "    directory=img_dir,\n",
    "    x_col='ImageName',\n",
    "    y_col='cellType',\n",
    "    target_size=(27, 27),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "multi_validation_generator = val_datagen.flow_from_dataframe(\n",
    "    dataframe=val_data,\n",
    "    directory=img_dir,\n",
    "    x_col='ImageName',\n",
    "    y_col='cellType',\n",
    "    target_size=(27, 27),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "multi_test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test_data,\n",
    "    directory=img_dir,\n",
    "    x_col='ImageName',\n",
    "    y_col='cellType',\n",
    "    target_size=(27, 27),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806eea1e-126e-49fa-baca-df99ec204a74",
   "metadata": {},
   "source": [
    "#### Function to plot learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128a4635-0a1f-4367-ac29-65547ba23142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(train_loss, val_loss, train_metric, val_metric, metric_name='F1-score'):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(train_loss, 'r--')\n",
    "    plt.plot(val_loss, 'b--')\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(train_metric, 'r--')\n",
    "    plt.plot(val_metric, 'b--')\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e90cb3-f6e3-4d7f-923b-3a6e7b517252",
   "metadata": {},
   "source": [
    "#### Baseline Models\n",
    "- The following is the baseline models for **isCancerous**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a483745-3103-47ec-854e-c246f728ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import Metric, Precision, Recall\n",
    "\n",
    "input_shape = (27, 27, 3)\n",
    "\n",
    "cnn_baseline = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=input_shape),\n",
    "\n",
    "    # Block 1\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Block 2\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Block 3\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "# Compile with precision and recall metrics\n",
    "cnn_baseline.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        Precision(name='precision'),\n",
    "        Recall(name='recall')\n",
    "        #F1Score(name='f1_score')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history_cnn = cnn_baseline.fit(\n",
    "    binary_train_generator,\n",
    "    steps_per_epoch=len(binary_train_generator),\n",
    "    validation_data=binary_validation_generator,\n",
    "    validation_steps=len(binary_validation_generator),\n",
    "    epochs=25,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Plot precision \n",
    "plot_learning_curve(\n",
    "    history_cnn.history['loss'],\n",
    "    history_cnn.history['val_loss'],\n",
    "    history_cnn.history['precision'],        \n",
    "    history_cnn.history['val_precision'],    \n",
    "    metric_name='Precision'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4948c38-00cb-4feb-aa7a-6c0eb4318d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (27, 27, 3)\n",
    "mlp_binary = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=input_shape),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "mlp_binary.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        Precision(name='precision'),\n",
    "        Recall(name='recall'),\n",
    "        #F1Score(name='f1_score')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history_mlp_binary = mlp_binary.fit(\n",
    "    binary_train_generator,\n",
    "    validation_data=binary_validation_generator,\n",
    "    epochs=25,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Precision\n",
    "plot_learning_curve(\n",
    "    history_mlp_binary.history['loss'],\n",
    "    history_mlp_binary.history['val_loss'],\n",
    "    history_mlp_binary.history['precision'],\n",
    "    history_mlp_binary.history['val_precision'],\n",
    "    metric_name='Precision'\n",
    ")\n",
    "\n",
    "# Recall\n",
    "plot_learning_curve(\n",
    "    history_mlp_binary.history['loss'],\n",
    "    history_mlp_binary.history['val_loss'],\n",
    "    history_mlp_binary.history['recall'],\n",
    "    history_mlp_binary.history['val_recall'],\n",
    "    metric_name='Recall'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ea43ec-270f-4fdd-b8a2-1f90151d30de",
   "metadata": {},
   "source": [
    "- The following is the baseline models for **cellType**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d5016a-0166-4b8f-9796-f5fda8c95767",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalPrecision(Precision):\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)  # softmax -> class index\n",
    "        return super().update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "class GlobalRecall(Recall):\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        return super().update_state(y_true, y_pred, sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f580c7-19af-4ee9-ab23-1f25802c5f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (27, 27, 3)\n",
    "# As it is a categorical variable requires unique number of classes\n",
    "num_classes = len(train_data['cellType'].unique())\n",
    "\n",
    "celltype_model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=input_shape),\n",
    "\n",
    "    # Block 1\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Block 2\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Block 3\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax'),\n",
    "])\n",
    "\n",
    "celltype_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[\n",
    "        GlobalPrecision(name='precision'),\n",
    "        GlobalRecall(name='recall')\n",
    "    ]\n",
    ")\n",
    "\n",
    "#  history_celltype = celltype_model.fit(\n",
    "#     multi_train_generator,\n",
    "#     validation_data=multi_validation_generator,\n",
    "#     epochs=25,\n",
    "#     verbose=0\n",
    "# )\n",
    "\n",
    "history_celltype = celltype_model.fit(\n",
    "    multi_train_generator,\n",
    "    steps_per_epoch=len(multi_train_generator),\n",
    "    validation_data=multi_validation_generator,\n",
    "    validation_steps=len(multi_validation_generator),\n",
    "    epochs=25,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "\n",
    "plot_learning_curve(\n",
    "    history_celltype.history['loss'],\n",
    "    history_celltype.history['val_loss'],\n",
    "    history_celltype.history['precision'],\n",
    "    history_celltype.history['val_precision'],\n",
    "    metric_name='Precision'\n",
    ")\n",
    "\n",
    "plot_learning_curve(\n",
    "    history_celltype.history['loss'],\n",
    "    history_celltype.history['val_loss'],\n",
    "    history_celltype.history['recall'],\n",
    "    history_celltype.history['val_recall'],\n",
    "    metric_name='Recall'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7b8f64-9384-4a02-8733-21921c0da756",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (27, 27, 3)\n",
    "mlp_multi = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=input_shape),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax'),\n",
    "])\n",
    "mlp_multi.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[\n",
    "        GlobalPrecision(name='precision'),\n",
    "        GlobalRecall(name='recall')\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_mlp_multi = mlp_multi.fit(\n",
    "    multi_train_generator,\n",
    "    validation_data=multi_validation_generator,\n",
    "    epochs=25,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "plot_learning_curve(\n",
    "    history_celltype.history['loss'],\n",
    "    history_celltype.history['val_loss'],\n",
    "    history_celltype.history['precision'],\n",
    "    history_celltype.history['val_precision'],\n",
    "    metric_name='Precision'\n",
    ")\n",
    "\n",
    "plot_learning_curve(\n",
    "    history_celltype.history['loss'],\n",
    "    history_celltype.history['val_loss'],\n",
    "    history_celltype.history['recall'],\n",
    "    history_celltype.history['val_recall'],\n",
    "    metric_name='Recall'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cab85a6",
   "metadata": {},
   "source": [
    "#### Handling Class Imbalance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91078a83-d156-49f6-abfc-19cdd881bd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "# Compute class weights for binary task (isCancerous)\n",
    "y_bin = train_data['isCancerous'].astype(int).values\n",
    "weights_bin = class_weight.compute_class_weight('balanced',\n",
    "                                                classes=np.unique(y_bin),\n",
    "                                                y=y_bin)\n",
    "class_weights_binary = {i: w for i, w in enumerate(weights_bin)}\n",
    "\n",
    "# Compute class weights for multiclass task (cellType)\n",
    "y_multi = train_data['cellType'].values\n",
    "classes_multi = np.unique(y_multi)\n",
    "weights_multi = class_weight.compute_class_weight('balanced',\n",
    "                                                  classes=classes_multi,\n",
    "                                                  y=y_multi)\n",
    "class_weights_multi = {cls: w for cls, w in zip(classes_multi, weights_multi)}\n",
    "\n",
    "# Oversample minority in train_data for binary task\n",
    "counts = train_data['isCancerous'].value_counts()\n",
    "max_count = counts.max()\n",
    "oversampled_dfs = []\n",
    "for cls, cnt in counts.items():\n",
    "    if cnt < max_count:\n",
    "        df_cls = train_data[train_data['isCancerous'] == cls]\n",
    "        oversampled_dfs.append(df_cls.sample(max_count - cnt, \n",
    "                                             replace=True, \n",
    "                                             random_state=42))\n",
    "train_balanced = pd.concat([train_data] + oversampled_dfs).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create augmented generator for binary task\n",
    "aug_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.1\n",
    ")\n",
    "binary_aug_generator = aug_datagen.flow_from_dataframe(\n",
    "    train_balanced,\n",
    "    directory=img_dir,\n",
    "    x_col='ImageName',\n",
    "    y_col='isCancerous',\n",
    "    target_size=(27, 27),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 1) Oversample minority cell-types in train_data\n",
    "counts_multi = train_data['cellType'].value_counts()\n",
    "max_ct_multi = counts_multi.max()\n",
    "oversampled_multi = []\n",
    "for cls, cnt in counts_multi.items():\n",
    "    if cnt < max_ct_multi:\n",
    "        df_cls = train_data[train_data['cellType'] == cls]\n",
    "        oversampled_multi.append(\n",
    "            df_cls.sample(max_ct_multi - cnt, replace=True, random_state=42)\n",
    "        )\n",
    "train_multi_balanced = pd.concat([train_data] + oversampled_multi) \\\n",
    "                       .reset_index(drop=True)\n",
    "\n",
    "# 2) Build an augmented generator for cell-type\n",
    "aug_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.1\n",
    ")\n",
    "\n",
    "multi_aug_generator = aug_datagen.flow_from_dataframe(\n",
    "    dataframe=train_multi_balanced,\n",
    "    directory=img_dir,\n",
    "    x_col='ImageName',\n",
    "    y_col='cellType',\n",
    "    target_size=(27, 27),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "# EarlyStopping for binary CNN (monitoring validation binary accuracy)\n",
    "early_stop_bin = EarlyStopping(\n",
    "    monitor='val_binary_accuracy',\n",
    "    mode='max',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# EarlyStopping for multiclass CNN (monitoring validation categorical accuracy)\n",
    "early_stop_multi = EarlyStopping(\n",
    "    monitor='val_categorical_accuracy',\n",
    "    mode='max',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Add augmented images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6c7874-f3ec-4015-94ea-75d0e3921af7",
   "metadata": {},
   "source": [
    "- **Class Weights** computes inverse‐frequency weights so the loss penalizes mistakes on rare classes more heavily; this keeps the model from simply predicting the majority label to minimize loss. It’s lightweight, but ensures the optimizer sees minority classes as equally important.\n",
    "- **Oversampling** replicates under-represented samples until each class has the same count, giving the model equal exposure to all categories. While it can risk over-fitting to duplicates, when paired with augmentation it effectively counteracts extreme imbalance.\n",
    "- **Data Augmentation** applies random rescaling, flips, rotations, and zooms at training time to create new variants of each image, boosting dataset size and diversity without new data collection. This forces the network to learn transformation-invariant features, improving real-world robustness.\n",
    "- **Early Stopping** tracks validation accuracy and halt training once it hasn’t improved for a set number of epochs, preventing the model from memorizing noise. Restoring the best weights guarantees you deploy the version with the highest true generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7568f508",
   "metadata": {},
   "source": [
    "#### Algorithm Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6a7ede-36e8-47a7-b33f-17af6c4690f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdb11236-3b46-4957-9710-af08952751b2",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc2d062-dda4-487d-860a-3917a65424f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Balanced binary CNN with fit_generator\n",
    "history_cnn_balanced = cnn_baseline.fit(\n",
    "    binary_aug_generator,\n",
    "    validation_data=binary_validation_generator,\n",
    "    epochs=25,\n",
    "    class_weight=class_weights_binary,\n",
    "    callbacks=[early_stop_bin],\n",
    "    verbose=0\n",
    ")\n",
    "plot_learning_curve(\n",
    "    history_cnn_balanced.history['loss'],\n",
    "    history_cnn_balanced.history['val_loss'],\n",
    "    history_cnn_balanced.history['accuracy'],\n",
    "    history_cnn_balanced.history['val_accuracy'],\n",
    "    metric_name='Binary Accuracy (CNN Balanced)'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3) Balanced multiclass CNN with fit_generator\n",
    "history_cnn_multi_balanced = celltype_model.fit(\n",
    "    multi_aug_generator,\n",
    "    validation_data=multi_validation_generator,\n",
    "    epochs=25,\n",
    "    class_weight=class_weights_multi,\n",
    "    callbacks=[early_stop_multi],\n",
    "    verbose=0\n",
    ")\n",
    "plot_learning_curve(\n",
    "    history_cnn_multi_balanced.history['loss'],\n",
    "    history_cnn_multi_balanced.history['val_loss'],\n",
    "    history_cnn_multi_balanced.history['categorical_accuracy'],\n",
    "    history_cnn_multi_balanced.history['val_categorical_accuracy'],\n",
    "    metric_name='Categorical Accuracy (CNN Multiclass Balanced)'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3288bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# Hyperparameters\n",
    "reg_lambda     = 0.001     # L2 strength\n",
    "drop_rate_conv = 0.25      # Dropout after conv blocks\n",
    "drop_rate_dense= 0.5       # Dropout before final dense\n",
    "\n",
    "# Define model\n",
    "cnn_baseline = tf.keras.Sequential([\n",
    "    # Block 1\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation='relu',\n",
    "        kernel_regularizer=regularizers.l2(reg_lambda),\n",
    "        input_shape=(27, 27, 3)),\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "    Dropout(drop_rate_conv),\n",
    "\n",
    "    # Block 2\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu',\n",
    "        kernel_regularizer=regularizers.l2(reg_lambda)),\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "    Dropout(drop_rate_conv),\n",
    "\n",
    "    # Classifier head\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu',\n",
    "        kernel_regularizer=regularizers.l2(reg_lambda)),\n",
    "    Dropout(drop_rate_dense),\n",
    "\n",
    "    # Output\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile\n",
    "cnn_baseline.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Multi clas cnn \n",
    "celltype_model = tf.keras.Sequential([\n",
    "    # Block 1\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation='relu',\n",
    "        kernel_regularizer=regularizers.l2(reg_lambda),\n",
    "        input_shape=(27, 27, 3)),\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "    Dropout(drop_rate_conv),\n",
    "\n",
    "    # Block 2\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu',\n",
    "        kernel_regularizer=regularizers.l2(reg_lambda)),\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "    Dropout(drop_rate_conv),\n",
    "\n",
    "    # Classifier head\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu',\n",
    "        kernel_regularizer=regularizers.l2(reg_lambda)),\n",
    "    Dropout(drop_rate_dense),\n",
    "\n",
    "    # Output\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "])\n",
    "\n",
    "# Compile\n",
    "celltype_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['categorical_accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ─── Train & Plot for Binary CNN with L2 + Dropout ───\n",
    "history_cnn_l2 = cnn_baseline.fit(\n",
    "    binary_aug_generator,\n",
    "    validation_data=binary_validation_generator,\n",
    "    epochs=25,\n",
    "    class_weight=class_weights_binary,\n",
    "    callbacks=[early_stop_bin],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "plot_learning_curve(\n",
    "    history_cnn_l2.history['loss'],\n",
    "    history_cnn_l2.history['val_loss'],\n",
    "    history_cnn_l2.history['accuracy'],\n",
    "    history_cnn_l2.history['val_accuracy'],\n",
    "    metric_name='Binary Accuracy (L2+Dropout)'\n",
    ")\n",
    "\n",
    "\n",
    "# ─── Train & Plot for Multiclass CNN with L2 + Dropout ───\n",
    "history_multi_l2 = celltype_model.fit(\n",
    "    multi_aug_generator,\n",
    "    validation_data=multi_validation_generator,\n",
    "    epochs=25,\n",
    "    class_weight=class_weights_multi,\n",
    "    callbacks=[early_stop_multi],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "plot_learning_curve(\n",
    "    history_multi_l2.history['loss'],\n",
    "    history_multi_l2.history['val_loss'],\n",
    "    history_multi_l2.history['categorical_accuracy'],\n",
    "    history_multi_l2.history['val_categorical_accuracy'],\n",
    "    metric_name='Categorical Accuracy (L2+Dropout)'\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a83bad",
   "metadata": {},
   "source": [
    "#### Final Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214f1e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 1) Helper functions to build models with given hyperparameters\n",
    "def create_binary_cnn(reg_lambda, drop_conv, drop_dense):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.Input(shape=(27,27,3)),\n",
    "        tf.keras.layers.Conv2D(32,(3,3),padding='same',activation='relu',\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(reg_lambda)),\n",
    "        tf.keras.layers.MaxPooling2D((2,2)),\n",
    "        tf.keras.layers.Dropout(drop_conv),\n",
    "\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu',\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(reg_lambda)),\n",
    "        tf.keras.layers.Dropout(drop_dense),\n",
    "\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def create_multi_cnn(reg_lambda, drop_conv, drop_dense, num_classes):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.Input(shape=(27,27,3)),\n",
    "        tf.keras.layers.Conv2D(32,(3,3),padding='same',activation='relu',\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(reg_lambda)),\n",
    "        tf.keras.layers.MaxPooling2D((2,2)),\n",
    "        tf.keras.layers.Dropout(drop_conv),\n",
    "\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu',\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(reg_lambda)),\n",
    "        tf.keras.layers.Dropout(drop_dense),\n",
    "\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['categorical_accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# # 2) Define your search space\n",
    "# param_grid = {\n",
    "#     'reg_lambda': [1e-4, 5e-4, 1e-3],\n",
    "#     'drop_conv' : [0.1, 0.25, 0.4],\n",
    "#     'drop_dense': [0.3, 0.5, 0.7]\n",
    "# }\n",
    "\n",
    "# # 3) Binary grid search\n",
    "# best_bin_acc = 0.0\n",
    "# best_bin_params = None\n",
    "\n",
    "# for reg in param_grid['reg_lambda']:\n",
    "#     for dconv in param_grid['drop_conv']:\n",
    "#         for ddense in param_grid['drop_dense']:\n",
    "#             model = create_binary_cnn(reg, dconv, ddense)\n",
    "#             history = model.fit(\n",
    "#                 binary_aug_generator,\n",
    "#                 validation_data=binary_validation_generator,\n",
    "#                 epochs=15,                        # shorter for speed\n",
    "#                 class_weight=class_weights_binary,\n",
    "#                 callbacks=[early_stop_bin],\n",
    "#                 verbose=0\n",
    "#             )\n",
    "#             val_acc = max(history.history['val_accuracy'])\n",
    "#             if val_acc > best_bin_acc:\n",
    "#                 best_bin_acc = val_acc\n",
    "#                 best_bin_params = {'reg_lambda':reg,\n",
    "#                                    'drop_conv':dconv,\n",
    "#                                    'drop_dense':ddense}\n",
    "\n",
    "# print(\"Best Binary CNN params:\", best_bin_params)\n",
    "# print(\"Best Binary val_accuracy:\", best_bin_acc)\n",
    "\n",
    "\n",
    "# # 4) Multiclass grid search\n",
    "# num_classes = train_data['cellType'].nunique()\n",
    "# best_multi_acc = 0.0\n",
    "# best_multi_params = None\n",
    "\n",
    "# for reg in param_grid['reg_lambda']:\n",
    "#     for dconv in param_grid['drop_conv']:\n",
    "#         for ddense in param_grid['drop_dense']:\n",
    "#             model = create_multi_cnn(reg, dconv, ddense, num_classes)\n",
    "#             history = model.fit(\n",
    "#                 multi_aug_generator,\n",
    "#                 validation_data=multi_validation_generator,\n",
    "#                 epochs=15,\n",
    "#                 class_weight=class_weights_multi,\n",
    "#                 callbacks=[early_stop_multi],\n",
    "#                 verbose=0\n",
    "#             )\n",
    "#             val_acc = max(history.history['val_categorical_accuracy'])\n",
    "#             if val_acc > best_multi_acc:\n",
    "#                 best_multi_acc = val_acc\n",
    "#                 best_multi_params = {'reg_lambda':reg,\n",
    "#                                      'drop_conv':dconv,\n",
    "#                                      'drop_dense':ddense}\n",
    "\n",
    "# print(\"Best Multiclass CNN params:\", best_multi_params)\n",
    "# print(\"Best Multiclass val_categorical_accuracy:\", best_multi_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161b7ec5",
   "metadata": {},
   "source": [
    "#### Final Models for isCancerous & cellType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f439877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Instantiate final models with your chosen hyperparameters ──\n",
    "bin_reg, bin_dconv, bin_ddense = 0.0001, 0.1, 0.7\n",
    "final_binary = create_binary_cnn(bin_reg, bin_dconv, bin_ddense)\n",
    "\n",
    "multi_reg, multi_dconv, multi_ddense = 0.001, 0.4, 0.3\n",
    "num_classes = train_data['cellType'].nunique()\n",
    "final_multi = create_multi_cnn(multi_reg, multi_dconv, multi_ddense, num_classes)\n",
    "\n",
    "# ── Train them ──\n",
    "history_bin = final_binary.fit(\n",
    "    binary_aug_generator,\n",
    "    validation_data=binary_validation_generator,\n",
    "    epochs=27,\n",
    "    class_weight=class_weights_binary,\n",
    "    callbacks=[early_stop_bin],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history_multi = final_multi.fit(\n",
    "    multi_aug_generator,\n",
    "    validation_data=multi_validation_generator,\n",
    "    epochs=27,\n",
    "    class_weight=class_weights_multi,\n",
    "    callbacks=[early_stop_multi],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ── Plot learning curves ──\n",
    "plot_learning_curve(\n",
    "    history_bin.history['loss'], history_bin.history['val_loss'],\n",
    "    history_bin.history['accuracy'], history_bin.history['val_accuracy'],\n",
    "    metric_name='Binary CNN Final'\n",
    ")\n",
    "\n",
    "plot_learning_curve(\n",
    "    history_multi.history['loss'], history_multi.history['val_loss'],\n",
    "    history_multi.history['categorical_accuracy'], history_multi.history['val_categorical_accuracy'],\n",
    "    metric_name='Multiclass CNN Final'\n",
    ")\n",
    "\n",
    "# ── (Optional) Classification reports ──\n",
    "from sklearn.metrics import classification_report\n",
    "binary_validation_generator.reset()\n",
    "y_pred_bin = (final_binary.predict(binary_validation_generator) > 0.5).astype(int).ravel()\n",
    "y_true_bin = binary_validation_generator.classes\n",
    "print(classification_report(y_true_bin, y_pred_bin,\n",
    "      target_names=list(binary_validation_generator.class_indices)))\n",
    "\n",
    "multi_validation_generator.reset()\n",
    "import numpy as np\n",
    "y_pred_multi = np.argmax(final_multi.predict(multi_validation_generator), axis=1)\n",
    "y_true_multi = multi_validation_generator.classes\n",
    "print(classification_report(y_true_multi, y_pred_multi,\n",
    "      target_names=list(multi_validation_generator.class_indices)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d94c5d",
   "metadata": {},
   "source": [
    "### Extra and Main data csv for cellType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65499329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Predict cell types for df_extra\n",
    "\n",
    "extra_generator = ImageDataGenerator(rescale=1./255, data_format='channels_last').flow_from_dataframe(\n",
    "    dataframe=df_extra,\n",
    "    directory=img_dir,\n",
    "    x_col='ImageName',    \n",
    "    y_col=None,\n",
    "    target_size=(27, 27),\n",
    "    batch_size=32,\n",
    "    class_mode=None,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "df_extra['cellType'] = np.argmax(final_multi.predict(extra_generator), axis=1)\n",
    "df_extra['isCancerous'] = '0'  # Placeholder\n",
    "\n",
    "# Merge with main data and save\n",
    "df_combined = pd.concat([df_main, df_extra], ignore_index=True)\n",
    "df_combined.to_csv('combined_data_with_predictions.csv', index=False)\n",
    "\n",
    "# 2. Split into Train / Validation / Test\n",
    "\n",
    "df_combined['cellType'] = df_combined['cellType'].astype(str)\n",
    "\n",
    "# First split: 80% train+val, 20% test\n",
    "trainval_df, test_df = train_test_split(\n",
    "    df_combined,\n",
    "    test_size=0.2,\n",
    "    stratify=df_combined['cellType'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Second split: from trainval, 80% train, 20% val\n",
    "train_df, val_df = train_test_split(\n",
    "    trainval_df,\n",
    "    test_size=0.2,\n",
    "    stratify=trainval_df['cellType'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 3. Data Generators\n",
    "\n",
    "batch_size = 32\n",
    "target_size = (27, 27)\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_gen = datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    directory=img_dir,\n",
    "    x_col='ImageName',\n",
    "    y_col='cellType',\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_gen = datagen.flow_from_dataframe(\n",
    "    dataframe=val_df,\n",
    "    directory=img_dir,\n",
    "    x_col='ImageName',\n",
    "    y_col='cellType',\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_gen = datagen.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    directory=img_dir,\n",
    "    x_col='ImageName',\n",
    "    y_col='cellType',\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 4. Class Weights and Model Training\n",
    "\n",
    "y_train = train_df['cellType']\n",
    "class_names = np.unique(y_train)\n",
    "weights = compute_class_weight('balanced', classes=class_names, y=y_train)\n",
    "class_weights_multi = {cls: weight for cls, weight in zip(class_names, weights)}\n",
    "\n",
    "# Instantiate and compile model\n",
    "multi_reg, multi_dconv, multi_ddense = 0.001, 0.4, 0.3\n",
    "num_classes = df_combined['cellType'].nunique()\n",
    "final_multi = create_multi_cnn(multi_reg, multi_dconv, multi_ddense, num_classes)\n",
    "\n",
    "early_stop_multi = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history_multi = final_multi.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=27,\n",
    "    class_weight=class_weights_multi,\n",
    "    callbacks=[early_stop_multi],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 5. Evaluation – Classification Report on Test Set\n",
    "\n",
    "test_gen.reset()  # Ensure prediction order matches labels\n",
    "y_pred_probs = final_multi.predict(test_gen)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = test_gen.classes\n",
    "class_labels = list(test_gen.class_indices.keys())\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=class_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d786733-c33c-49ba-8cd6-832ec9863ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
